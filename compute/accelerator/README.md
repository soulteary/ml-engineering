# 加速器

计算加速器是机器学习训练中的工作马。最初只有GPU，但现在也有了TPU、IPU、FPGA、HPU、QPU、RDU等新型处理器不断被发明出来。

存在两种主要的机器学习任务：训练和推理。还有一种微调任务，通常与训练相同，除非执行了一种轻量级的[LORA风格](https://arxiv.org/abs/2106.09685)的微调，这种微调需要显著较少的时间和资源。后者所需的资源和时间比正常的微调要少得多。

在语言模型中，推断过程以序列方式进行，一次生成一个令牌。这意味着需要在数千次小小的矩阵乘法（或GEMM）调用中重复相同的`前向`操作。这可以在加速器上完成，如GPU，也可以在一些最新的CPU上高效地完成。

在训练过程中，整个序列长度在一次巨大的矩阵乘法操作中被处理。如果序列长度为4千个单位，那么同样的模型的训练将需要能够处理至少4千倍于推断所需运算量的计算单元，并且速度也要足够快。加速器在这方面表现出色。事实上，矩阵越大，计算效率越高。

另一个重要的计算差异在于，虽然训练和推理都需要执行相同总量的矩阵乘法运算，但在仅用于训练的后向传播过程中，为了计算相对于输入和权重的梯度，还需要额外执行两倍的矩阵乘法运算。此外，如果在后向传播中使用了激活函数的重计算技术，还会额外执行一次`前向`操作。因此，训练过程需要的矩阵乘法运算量大约是推理的三到四倍。

## 小节

- [NVIDIA GPU故障排除](nvidia/debug.md)


## 对高端加速器的鸟瞰图

尽管未来可能会有所变化，但截至本文撰写时，市场上并没有太多的高端加速器选择，而且大多数云服务提供商提供的都是类似的几种加速器型号。

GPU：
- 目前，机器学习和高性能计算环境正在从NVIDIA的A100过渡到H100，这一转换预计将在几个月内完成，因为NVIDIA的GPU供应仍然紧张。
- AMD的MI250开始零星出现在一些地方，但其广泛可用性尚不清楚。MI300X承诺将于2024年3月开始在二级云供应商处提供。

HPU：
- Intel的Gaudi2已经开始缓慢地在Intel的云平台上出现，它们有一个庞大的产品线。这些也在超大规模数据中心的内置环境中通过Supermicro和WiWynn等公司提供。

IPU：
- Graphcore提供了其IPU解决方案。您可以通过Paperspace的云笔记本尝试这些。

TPU：
- Google的TPUs当然也是可用的，但由于只能租赁使用，且软件与其他硬件平台相比不太容易移植，许多（大多数？）开发人员仍留在GPU领域，因为他们不想被锁定在一个由谷歌垄断的硬件上。

Pod和机架级别：
- Cerebras的WaferScale Engine（WSE）
- SambaNova的数据规模系统
- 以及数十种不同的包含上述GPU和其他超级快的互连组件的pod和机架配置。

这就是截至2024年第一季度的情况。


## 词汇表

- CPU：中央处理器
- FPGA：现场可编程门阵列
- GCD：图形计算核心
- GPU：图形处理器
- HBM：高带宽内存
- HPC：高性能计算
- HPU：Habana Gaudi人工智能处理器单元
- IPU：智能处理单元
- MME：矩阵乘积引擎
- QPU：量子处理器单元
- RDU：可重构数据流单元
- TPU：张量处理器单元

## 我们需要理解的最重要的事情

我将多次在本书中提出以下观点——仅仅购买/租用最昂贵的加速器并不足以确保获得高的投资回报率（ROI）。

衡量ML培训中高ROI的两个关键指标是：
1. 训练完成的速度，因为在竞争激烈的ML市场中，如果训练时间超出计划的两三倍，您的模型可能会在发布之前变得过时——时间就是一切。
2. 训练模型总共花费的钱，因为如果训练时间超出计划的两三倍，你将会多花两三倍的费用。

如果不仔细匹配工作负载的需求，即使购买了/租用了性能最好的硬件，也可能会导致大量的时间和金钱损失。最关键的部分是网络，其次是存储，而CPU和CPU内存则是影响最小的部分（对于典型的培训工作负载来说，任何CPU限制都可以通过多个`DataLoader`工作者来补偿）。

如果您是在云端租用计算资源，通常没有自由选择硬件的权利，硬件要么已经确定，要么只能在有限的范围内替换某些部件。在这种情况下，如果选择的云服务提供商不能提供足够合适的硬件，可能需要考虑其他提供商。

如果你自己购买服务器，我强烈建议在进行深入研究后再做出购买决策。

除了硬件之外，您还需要能够有效利用硬件的软件。

我们将在本章及其他章节讨论硬件和软件方面的内容。您可以先阅读[这里](../../training/performance)和[这里](../../training/model-parallelism)的内容。



## 加速器特性关注点

让我们以NVIDIA A100的规格作为参考点来进行讨论。

![nvidia-a100-spec](images/nvidia-a100-spec.png)

[来源](https://www.nvidia.com/en-us/data-center/a100/)

### TFLOPS

正如前面提到的，大多数ML训练和推理的工作都涉及矩阵乘法。简而言之，矩阵乘法是由一系列的乘法和加法组成的。每个这样的计算都可以计数并定义芯片每秒能执行的浮点运算数量。

这是评估加速器的重要特征之一。TFLOPS表示芯片每秒钟可以执行多少万亿次的浮点运算。数值越大越好。根据数据的类型，有不同的定义。以下是来自A100规格的一些理论峰值TFLOPS值：

| 数据类型\ TFLOPS     | 无稀疏 | 有稀疏 |
| :------------------- | ------: | ------: |
| FP32                 |   19.5  | n/a     |
| Tensor Float 32 (TF32)|  156.0  |   312   |
| BFLOAT16 Tensor Core |  312.0  |   624   |
| FP16 Tensor Core     |  312.0  |   624   |
| FP8 Tensor Core      |  624.0  |  1248   |
| INT8 Tensor Core     |  624.0  |  1248   |

注意：

* INT8由于不是浮点运算，所以它的TFLOPS是以Tera Operations的形式测量的。

* FLOPS这个术语既可以用来表示总的浮点运算次数（例如，当计算单个Transformer迭代所需要的FLOPS总量时），也可以用来表示每秒浮点运算次数——所以在不同上下文中需要注意区分。在查看加速器的规格时，它几乎总是指每秒的定义。而在讨论模型架构时，它通常只是代表总的浮点运算次数。

因此，我们可以看到INT8的速度是BF16的两倍，而BF16又是TF32的两倍。

此外，TFLOPS会随着矩阵大小的增加呈现出非线性增长，这是因为[Tile和Wave量化效应](../../training/performance#tile-and-wave-quantization)的影响。


#### TFLOPS对比表格

下面是一个基于不同数据类型的理论峰值TFLOPS值的对比表格，针对的是当前市场上的高端加速器（不包括稀疏模式）。按照bf16排序。

| 加速器\ TFLOPS | fp32 | tf32 | fp16 | bf16 | fp8 | int8 |
| :------------- | ---: | ---: | ---: | ---: | --: | ---: |
| AMD MI300X     | 163.4 | 653.7 | 1300 | 1300 | 2600 | 2600 |
| NVIDIA H100 SXM|  67.0 | 494.5 |  989 |  989 | 1979 | 1979 |
| NVIDIA H200 SXM|  67.0 | 494.5 |  989 |  989 | 1979 | 1979 |
| NVIDIA H100 PCIe|  51.0 | 378.0 |  756 |  756 | 1513 | 1513 |
| Intel Gaudi2   | V     | V     | V     | V     | V    | V    |
| Google TPU v5p | X     | X     | X     | 459   | X    | 918  |
| AMD MI250X     |  47.9 | X     |  383 |  383 | X    |  383 |
| NVIDIA L40S    |  91.6 | 183.0 |  362 |  362 |  733 |  733 |
| AMD MI250      |  45.3 | X     |  362 |  362 | X    |  362 |
| NVIDIA A100 SXM|  19.5 | 156.0 |  312 |  312 | X    |  624 |
| Google TPU v4  | X     | X     | X     | 275   | X    | X    |
| Google TPU v5e | X     | X     | X     | 197   | X    | 394  |

注解：

* int8的测量单位是Tera Operations，因为它不是一个浮点运算。

* Intel Gaudi2尚未公布其TFLOPS规格（截至本文写作时），但它确实支持FP32、TF32、BF16、FP16 & FP8、INT8和INT16。一篇[博客文章](https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators)报道了大约400TFLOPS的fp16/bf16混合精度模式的测试结果——但这只是一个粗略估计，因为官方并未正式公布这些数字。合理的猜测是它在600-1000TFLOPS的范围之内。

* 如果某个加速器公布的TFLOPS高于另一款加速器，并不意味着前者就更快。这些是理论值，在实际应用中永远无法达到，实际的TFLOPS效率（HFU）因厂商和加速器架构的不同而有很大差异。


#### 最大可达FLOPS

理论峰值FLOPS是加速器规格中公开的信息。它是这样计算的：

`理论FLOPS = 计算单元时钟频率 * 每个计算单元每周期执行的浮点操作数 * 计算单元的数量`

其中：
- `计算单元时钟频率` – 计算单元每秒能产生多少个时钟脉冲（Hz）
- `每个计算单元每周期执行的浮点操作数` – 计算单元能在每个时钟周期内执行多少个操作
- `计算单元的数量` – 设备中有多少个计算单元

问题是，广告宣传的理论峰值FLOPS是非常理论化的，即使在理想条件下也无法实现。实际上，每个加速器都有自己的实际可达FLOPS，但这些信息不会公开，社区成员有时会在论坛上分享他们所能实现的最高FLOPS，但我还没有找到任何官方报告。

为了提供一个具体的例子，A100的理论峰值BF16 FLOPS是312 TFLOPS。直到FlashAttention的出现，人们普遍认为150 TFLOPS是fp16/bf16混合精度的极限。有了FlashAttention，这个数字提高到了大约180 TFLOPS。这些都是针对LLM训练场景下的基准测试得到的，其中包括了网络和IO的开销，因此实际的峰值性能可能在200到300 TFLOPS之间。

理论上，应该有可能通过动态调整最大尺寸的矩阵相乘来精确找出实际的最大可达FLOPS。

XXX：编写一个小程序来动态发现完美对齐的最大尺寸矩阵相乘，并根据[Tile和Wave量化效应](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization)调整形状大小，以便基准测试不受硬编码限制。


### 加速器内存容量和速度

加速器使用[高带宽内存](https://zh.wikipedia.org/wiki/%E9%AB%98%E5%8F%AF%E9%9D%9E%E5%BA%A6%E5%86%85%E5%AD%98)（HBM），这是一种三维版的SDRAM内存。例如，A100-SXM配备了HBM2，传输速率为1.6 TBps，而H100-SXM则配备了更快的HBM3，传输速率达到了3.35 TBps。

以下是一些内存规格：

| 代数 | 数据速率(Gbps) | 每设备带宽(GBps) | 堆栈高度 | 最大DRAM容量(GB) | 最大设备容量(GB) |
| :---: | -------------: | ---------------: | --------: | ---------------: | ---------------: |
| HBM   |        1.0     |             128  |       8   |              2   |              16  |
| HBM2  |        2.0     |             256  |       8   |              2   |              16  |
| HBM2e |        3.6     |             461  |      12   |              3   |              36  |
| HBM3  |        6.4     |             819  |      16   |              4   |              64  |
| HBM3e |        9.6     |            1229  |      16   |              4   |              64  |

由于HBM是一种内存堆叠结构，堆栈高度决定了有多少个DRAM芯片组成一个设备。

一般来说，加速器拥有更多的本地内存是有益的。在任何给定时间内，大部分模型权重都不会同时被使用，而是等待轮换进行处理。因此，较大的内存允许更多模型参数驻留在加速器内存中，随时可供访问和更新。当内存不足时，模型可能不得不分割到多个加速器上，或者卸载到CPU和/或磁盘上。

以下是近期高端加速器的内存规格对比，按内存容量降序排列：

| 加速器          | 内存(GBs) | 类型  | 带宽(TBps) |
| :-------------- | --------: | :---- | ---------: |
| AMD MI300X      |       192 | HBM3  |       5.30 |
| NVIDIA GH200 SXM (2) |       141 | HBM3e |       4.80 |
| NVIDIA H200 SXM |       141 | HBM3e |       4.80 |
| AMD MI250        |       128 | HBM2e |       3.28 |
| AMD MI250X       |       128 | HBM2e |       3.28 |
| NVIDIA GH200 SXM (1) |        96 | HBM3  |       4.00 |
| Intel Gaudi2    |        96 | HBM2e |       2.45 |
| Google TPU v5p  |        95 | HBM2e |       4.80 |
| NVIDIA H100 SXM |        80 | HBM3  |       3.35 |
| NVIDIA A100 SXM |        80 | HBM2e |       2.00 |
| NVIDIA H100 PCIe |        80 | HBM3  |       3.35 |
| NVIDIA L40S     |        48 | GDDR6 |       0.86 |
| Google TPU v4   |        32 | HBM2  |       2.40 |
| Google TPU v5e  |        16 | HBM2  |       1.60 |

注解：

* 我未将“NVIDIA H100双NVL”纳入比较，因为它相当于两个GPU，因此在上述表格中会有不公平的优势（尽管它实际上是两个独立的GPU）。

内存速度（带宽）也非常重要，因为如果不够快，计算单元就会闲置等待数据传输，从而降低整体性能（严重情况下甚至可能导致系统崩溃，尽管设计上应避免这种情况发生）。




### 热量

这对于自行采购硬件的人来说尤为重要，而对于那些在云上租用资源的用户来说，云服务商会负责保持适当的冷却措施。

加速器产生的热量是一个值得关注的因素，因为如果散热不当，它们会降低运行频率以减少发热（极端情况下甚至会关机，尽管设计上应尽量避免这种情况）。




## 大型语言模型/视觉模型工作负载的高端加速器

### 云计算和本地部署的加速器集群

大多数常见的高端加速器既可以作为云服务租用，也可以直接购买：

NVIDIA：
- [H100](https://www.nvidia.com/en-us/data-center/h100) – 比A100快2-3倍（半精度下），fp8下快6倍。自2023年第四季度以来已在所有一级计算云上可用。
- [GH200](https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/) – 两个芯片集成在一块卡上：（1）H100带有96 GB HBM3或144 GB HBM3e + （2）Grace CPU带有624 GB RAM。首批产品已开始交付。不要将其与H200混淆，后者是完全不同的卡。
- [L40S](https://www.nvidia.com/en-us/data-center/l40s/) – 一种功能强大的新卡，据称价格不到H100的一半，并且在某些方面比A100还要强大。预计将于2024年中旬左右上市。
- [H200](https://www.nvidia.com/en-us/data-center/h200/) – 与H100基本相同，但具有更大、更快的内存！预计将于2024年中旬左右上市。
- [A100](https://www.nvidia.com/en-us/data-center/a100/#specifications) – 供货充足，但逐渐过时。尽管成本较低，它仍然是很好的选择。

AMD：
- [MI250](https://www.amd.com/en/products/accelerators/instinct/mi200/mi250.html) ~= A100 – 在少数云上有售
- [MI300X](https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html) ~= H100 – 刚刚崭露头角，主要见于二级云服务（很多新的初创企业）。

Intel：
- [Gaudi2](https://habana.ai/products/gaudi2/) ~= H100 – [规范](https://docs.habana.ai/en/latest/Gaudi_Overview/Gaudi_Architecture.html) – [目前，在云.google.com上仅有少量可用，且有一长串等候名单，据说有望在2024年第一季度减少](https://cloud.google.com)。AWS上有较旧的Gaudi1版本[通过DL1实例](https://aws.amazon.com/ec2/instance-types/dl1/)提供。它还可在超大规模数据中心的内置环境中通过Supermicro和WiWynn等公司提供。

Graphcore：
- [IPU](https://www.graphcore.ai/products/ipu) – 可通过[Paperspace](https://www.paperspace.com/graphcore)提供。最新产品MK2（C600）仅有0.9GB SRAM per card，因此尚不清楚如何用于任何ML任务——即使是小模型的推理也不太可能完全适合其模型权重。不过，Graphcore似乎有一些即将到来的新技术，据我所知，我们应该很快就能了解更多信息。这里是对IPU工作的良好解释[《IPUS 101》](https://thytu.com/posts/ipus-101/)。

SambaNova：
- [DataScale SN30](https://sambanova.ai/products/datascale/)


### 本地部署加速器集群

Cerebras：
- [集群](https://www.cerebras.net/product-cluster/)
- [系统](https://www.cerebras.net/product-system/)
基于WaferScale Engine（WSE）构建。




### 仅云服务的解决方案

这些只能在云上使用：

Google：
- [TPUs](https://cloud.google.com/tpu)，[规范](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm) – 锁定，一旦采用很难切换到其他供应商（如NVIDIA->AMD）。

Cerebras：
- [云服务](https://www.cerebras.net/product-cloud/)




### 获取最佳价格的方法

请记住，广告标价通常是谈判后的价格基础，只要您愿意批量购买或在1-3年内签订长期合同。实际上，您最终支付的价格可能是广告标价的许多倍。一些云服务提供商已经在他们的网站上包含了折扣，但如果您直接与销售团队协商，可能会得到更好的价格或其他优惠条件。此外，如果贵公司的背后有风险资本投资者，提及这一点可能会有帮助，因为这表明您在未来可能会购买更多的计算资源，从而增加了进一步降价的可能性。

一级云服务提供商的定价往往较高，因此可以考虑转向二级云服务提供商寻求更有竞争力的价格。一级云服务提供商包括AWS、OCI、Azure和GCP。

在寻找解决方案时，请记得不仅要看加速器的性能，还要看其配套的基础设施，比如高速[节点间网络](../../network#inter-node-networking)和[存储系统](../../storage)。如果没有这些基础设施的支持，昂贵的加速器可能会空闲等待数据到达，从而浪费资金和时间。
