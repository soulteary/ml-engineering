# 存储：文件系统和输入输出（IO）

## 机器学习中的三个不同的IO需求

在机器学习的任务中，有三种不同的IO需求：

1. 你需要能够快速地喂数据给DataLoader（超级快的读取，不关心快速的写入）——这要求系统能持续承受高负载数小时或几天。
2. 你需要能够快速地写入检查点（checkpoint）（超级快的写入，相对较快的读取，因为你可能会多次恢复训练）——这要求系统能在短时间内提供极高的写入吞吐量（burst writing），以避免长时间阻塞训练进程（除非你使用某种CPU卸载技术来迅速解除对训练的阻塞）。
3. 你需要能够加载和维护你的代码库（medium speed for both reading and writing）——这也需要共享，因为所有节点都需要看到相同的代码库——由于这些操作只在启动或恢复时发生，因此它们发生的频率较低。

如你所见，这三个需求对于速度和可持续负载的要求非常不同，因此理想情况下你会希望拥有三个分别针对特定用例进行优化的独立文件系统。

如果你资金无限，当然可以购买一个既能实现超快读写、又能连续工作几天的解决方案。但对于我们大多数人来说，这可能不是一种选择，所以我们可能需要在成本上做出妥协，选择多个更便宜的分区方案。



## 术语表

- NAS：网络连接存储
- SAN：存储区域网路
- DAS：直接附加存储
- NSD：网络共享磁盘
- OSS：对象存储服务器
- MDS：元数据服务器
- MGS：管理服务器



## 选择哪个文件系统

**分布式并行文件系统是最快的解决方案**

分布式并行文件系统显著提高了性能，当数百到数千个客户端同时访问共享存储时，这种提升尤为明显。此外，它们还能帮助减少热点问题（某些数据的访问频率远高于其他数据）。

我有经验的两款表现优异的并行文件系统是：

- [Lustre FS](https://www.lustre.org/)（开源）([维基百科](https://wiki.lustre.org/Main_Page))
- [GPFS](https://en.wikipedia.org/wiki/GPFS)（IBM出品，最近改名为IBM Storage Scale，之前叫过IBM Spectrum Scale）

这两套解决方案都有超过二十年的历史了。它们都符合POSIX标准。创建和管理这样的文件系统并不简单，你需要先设置一个专用的集群，包含多个仅用于运行这些文件系统的CPU虚拟机——这与云服务商提供的内置“一键式”解决方案形成了鲜明对比。然而，尽管配置复杂性较高，但一旦部署完成，这些高级文件系统通常会带来显著的性能提升。例如，在法国的JeanZay高性能计算中心，我们就曾将2.3 TB的检查点在384个进程中并行保存，并在40秒内完成了这一过程！这是惊人的高速，而且使用的正是GPFS和NVME驱动器。

案例研究：在JeanZay HPC，为了应对大量的小型Conda环境安装带来的挑战，我们需要请求一个专门的专用分区来进行Conda环境的安装。这是因为我们的常规GPFS分区被配置为16 MB的块大小，这对于处理4 KB大小的文件而言不太合适。Python模块的众多小文件可能导致这些问题。